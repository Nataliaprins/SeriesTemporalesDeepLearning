Modelar una serie de tiempo con MLP
===========================================

.. code:: ipython3

    import pandas as pd 
    # Leer el archivo CSV
    file_path = "/Users/nataliaacevedo/SeriesTemporalesDeepLearning/notebooks/Series con ML/global_weather_data_2015_2024.csv"
    data = pd.read_csv(file_path, encoding='latin1')
    usa_data = data[data['country'] == 'USA']
    new_york_data = usa_data[usa_data['city'] == 'New York']
    new_york_data.set_index('date', inplace=True)
    new_york_tavg = new_york_data['tavg']
    print(new_york_tavg.head())



.. parsed-literal::

    date
    2015-01-02 00:00:00    2.6
    2015-01-03 00:00:00    0.9
    2015-01-04 00:00:00    6.6
    2015-01-05 00:00:00    5.4
    2015-01-06 00:00:00   -6.4
    Name: tavg, dtype: float64


Preprocesamiento
^^^^^^^^^^^^^^^^
Extracci√≥n de caracter√≠sticas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    # Crear las ventanas de tiempo para series temporales
    import numpy as np  
    
    def create_time_windows(series, window_size):
        X, y = [], []
        for i in range(len(series) - window_size):
            X.append(series.iloc[i:i + window_size].values)  # Usar .iloc para acceder por posici√≥n
            y.append(series.iloc[i + window_size])          # Usar .iloc para el valor objetivo
        return np.array(X), np.array(y)
    
    # Crear ventanas de 7 d√≠as
    X, y = create_time_windows(new_york_tavg, 7)
    
    X = np.array(X)
    y = np.array(y).flatten()  # Aseg√∫rate de que y sea 1D
    
    print("Ventanas de entrada (X):")
    print(X.shape)
    print(X)
    print("Valores objetivo (y):")
    print(y.shape)
    print(y)



.. parsed-literal::

    Ventanas de entrada (X):
    (3642, 7)
    [[  2.6   0.9   6.6 ...  -6.4  -7.1 -11.1]
     [  0.9   6.6   5.4 ...  -7.1 -11.1  -5.1]
     [  6.6   5.4  -6.4 ... -11.1  -5.1  -6.6]
     ...
     [  6.8   2.4  -0.3 ...  -7.7  -0.4   1.1]
     [  2.4  -0.3  -6.6 ...  -0.4   1.1  -1.3]
     [ -0.3  -6.6  -7.7 ...   1.1  -1.3  -0.6]]
    Valores objetivo (y):
    (3642,)
    [-5.1 -6.6 -5.1 ... -1.3 -0.6  5.1]


**Ventana de entrada (X):**

Cada ventana contiene window_size valores consecutivos de la serie.

Por ejemplo, si ``window_size = 3`` y la serie es [1, 2, 3, 4, 5], las
ventanas ser√°n:

``X[0] = [1, 2, 3]``

``X[1] = [2, 3, 4]``

**Valor objetivo (y):**

El valor objetivo es el siguiente valor en la serie despu√©s de la
ventana. En el ejemplo anterior: ``y[0] = 4`` (el valor despu√©s de [1,
2, 3]) ``y[1] = 5`` (el valor despu√©s de [2, 3, 4])

**Sin fuga de datos:**

No se utiliza informaci√≥n del futuro para construir las ventanas de
entrada (X).

El valor objetivo (y) siempre est√° fuera de la ventana de entrada y
corresponde a un punto en el futuro. Consideraciones adicionales:

**Orden temporal:** Aseg√∫rate de que la serie est√© correctamente
ordenada por tiempo antes de aplicar esta funci√≥n.

**Normalizaci√≥n:** Si planeas normalizar los datos, hazlo despu√©s de
dividir los datos en ventanas para evitar fuga de informaci√≥n entre las
ventanas.

Escalar o Normalizar la serie
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Recomendaci√≥n pr√°ctica para redes neuronales:**

Para series financieras o ruidosas ‚Üí RobustScaler o StandardScaler.

Para series con crecimiento o escala amplia ‚Üí Log + MinMaxScaler.

Para LSTM/GRU/CNN ‚Üí mantener datos en rango [‚àí1, 1] mejora la
convergencia.

+--------------------+----------+----------+------------------+-------------------+
| M√©todo             | Robustez | Mantiene | Ideal para       | Precauciones      |
|                    | a        | orden    |                  |                   |
|                    | outliers | temporal |                  |                   |
+====================+==========+==========+==================+===================+
| **Min‚ÄìMax**        | ‚ùå       | ‚úÖ       | Datos acotados,  | Sensible a        |
|                    |          |          | sin picos        | extremos          |
+--------------------+----------+----------+------------------+-------------------+
| **StandardScaler** | ‚ö†Ô∏è       | ‚úÖ       | Series estables  | Puede amplificar  |
|                    |          |          |                  | outliers          |
+--------------------+----------+----------+------------------+-------------------+
| **RobustScaler**   | ‚úÖ       | ‚úÖ       | Series           | Reduce            |
|                    |          |          | financieras o    | sensibilidad a    |
|                    |          |          | ruidosas         | cambios suaves    |
+--------------------+----------+----------+------------------+-------------------+
| **Log Transform**  | ‚úÖ       | ‚úÖ       | Datos positivos, | No usar con       |
|                    |          |          | crecimiento      | valores negativos |
|                    |          |          | exponencial      |                   |
+--------------------+----------+----------+------------------+-------------------+
| **Power            | ‚úÖ       | ‚úÖ       | Distribuciones   | Requiere ajuste   |
| Transform**        |          |          | sesgadas         | cuidadoso         |
+--------------------+----------+----------+------------------+-------------------+
| **Incremental      | ‚úÖ       | ‚úÖ       | Series en        | Precisi√≥n menor   |
| Scaling**          |          |          | streaming        | si cambian las    |
|                    |          |          |                  | estad√≠sticas      |
+--------------------+----------+----------+------------------+-------------------+
| **Per-variable     | ‚úÖ       | ‚úÖ       | Series           | Debe aplicarse    |
| Scaling**          |          |          | multivariadas    | coherentemente al |
|                    |          |          |                  | predecir          |
+--------------------+----------+----------+------------------+-------------------+

.. code:: ipython3

    from sklearn.preprocessing import MinMaxScaler
    
    # Crear instancias de MinMaxScaler
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    # Ajustar y transformar X
    X_scaled = scaler_X.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
    
    # Ajustar y transformar y
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    print("X escalado:")
    print(X_scaled)
    print("y escalado:")
    print(y_scaled)


.. parsed-literal::

    X escalado:
    [[0.34051724 0.30387931 0.42672414 ... 0.14655172 0.13146552 0.04525862]
     [0.30387931 0.42672414 0.40086207 ... 0.13146552 0.04525862 0.17456897]
     [0.42672414 0.40086207 0.14655172 ... 0.04525862 0.17456897 0.14224138]
     ...
     [0.43103448 0.3362069  0.27801724 ... 0.11853448 0.27586207 0.30818966]
     [0.3362069  0.27801724 0.14224138 ... 0.27586207 0.30818966 0.25646552]
     [0.27801724 0.14224138 0.11853448 ... 0.30818966 0.25646552 0.27155172]]
    y escalado:
    [0.17456897 0.14224138 0.17456897 ... 0.25646552 0.27155172 0.39439655]


Construcci√≥n del modelo MLP
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. code:: ipython3

    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Input

    #fijar el valor semilla para reproducibilidad
    import tensorflow as tf
    tf.random.set_seed(42)

    
    # Crear el modelo MLP
    model = Sequential([
        Input(shape=(X.shape[1],)),  # Define expl√≠citamente la forma de entrada
        Dense(32, activation='relu'),  # Capa densa con 32 neuronas
        Dense(1)  # Capa de salida con 1 neurona (predicci√≥n para 1 d√≠a)
    ])
    
    # Compilar el modelo
    model.compile(optimizer='adam', loss='mse', metrics=['mse'])

.. code:: ipython3

    # Entrenar el modelo
    history = model.fit(X_scaled, y_scaled, epochs=500, batch_size=32, validation_split=0.2, verbose=1)


.. parsed-literal::

    Epoch 1/500
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 2ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0085 - val_mse: 0.0085
    Epoch 2/500
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 773us/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0066 - val_mse: 0.0066
    Epoch 3/500
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 766us/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0054 - val_mse: 0.0054
    Epoch 4/500
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 823us/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0048 - val_mse: 0.0048
    Epoch 5/500
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 773us/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0042 - val_mse: 0.0042
    ...
    Epoch 500/500 
    [1m92/92[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 733us/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0036 - val_mse: 0.0036


.. code:: ipython3

    import matplotlib.pyplot as plt
    
    # Extraer las p√©rdidas de entrenamiento y validaci√≥n
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    # Crear la gr√°fica
    plt.figure(figsize=(10, 6))
    plt.plot(train_loss, label='P√©rdida de entrenamiento')
    plt.plot(val_loss, label='P√©rdida de validaci√≥n')
    plt.xlabel('√âpocas')
    plt.ylabel('P√©rdida')
    plt.title('P√©rdida de entrenamiento y validaci√≥n')
    plt.legend()
    plt.grid()
    plt.show()



.. image:: mlp_model_files/mlp_model_9_0.png


.. code:: ipython3

    # Realizar predicciones
    y_pred = model.predict(X_scaled)
    # Invertir la escala de las predicciones
    y_pred_rescaled = scaler_y.inverse_transform(y_pred)
    



.. parsed-literal::

    [1m114/114[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 498us/step


.. code:: ipython3

    # Graficar valores reales vs predichos
    plt.figure(figsize=(10, 6))
    plt.plot(y, label='Valores reales', alpha=0.7)
    plt.plot(y_pred_rescaled, label='Valores predichos', alpha=0.7)
    plt.xlabel('√çndice')
    plt.ylabel('Valor')
    plt.title('Valores reales vs predichos')
    plt.legend()
    plt.grid()
    plt.show()



.. image:: mlp_model_files/mlp_model_11_0.png


.. code:: ipython3

    # Extraer los residuos
    residuals = y - y_pred_rescaled.flatten()
    print(residuals.shape)


.. parsed-literal::

    (3642,)


.. code:: ipython3

    plt.figure(figsize=(10, 6))
    plt.plot(residuals, label='Residuales', alpha=0.7, linestyle='', marker='o')  # Cambiar l√≠neas por puntos
    plt.xlabel('√çndice')
    plt.ylabel('Valor residual')
    plt.title('Residuales de la predicci√≥n')
    plt.axhline(0, color='red', linestyle='--', linewidth=1)
    plt.legend()
    plt.grid()
    plt.show()



.. image:: mlp_model_files/mlp_model_13_0.png



Optimizaci√≥n de Hiperpar√°metros
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

+------------------------------+---------------------------------------+
| Escenario                    | Recomendado                           |
+==============================+=======================================+
| Modelos cl√°sicos (SVM,       | ``GridSearchCV``,                     |
| Random Forest, XGBoost)      | ``RandomizedSearchCV``, ``Optuna``,   |
|                              | ``HyperOpt``                          |
+------------------------------+---------------------------------------+
| Modelos de Deep Learning     | ``Keras Tuner``, ``Optuna``,          |
| (Keras/TensorFlow)           | ``Ray Tune``                          |
+------------------------------+---------------------------------------+
| B√∫squeda escalable o         | ``Ray Tune``, ``BOHB``,               |
| distribuida                  | ``Optuna + Ray``                      |
+------------------------------+---------------------------------------+
| AutoML / pipelines           | ``TPOT``, ``Auto-Sklearn``,           |
| autom√°ticos                  | ``AutoKeras``                         |
+------------------------------+---------------------------------------+
| Exploraci√≥n evolutiva o      | ``Nevergrad``, ``DEAP``, ``PyGAD``    |
| heur√≠stica                   |                                       |
+------------------------------+---------------------------------------+

.. code:: ipython3

    from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
    from sklearn.base import BaseEstimator, RegressorMixin
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.optimizers import Adam, SGD
    import numpy as np
    
    # Crear un wrapper personalizado para Keras
    class KerasRegressor(BaseEstimator, RegressorMixin):
        def __init__(self, optimizer='adam', neurons=64, batch_size=32, epochs=10, verbose=0):
            self.optimizer = optimizer
            self.neurons = neurons
            self.batch_size = batch_size
            self.epochs = epochs
            self.verbose = verbose
            self.model = None
    
        def build_model(self):
            model = Sequential([
                Dense(self.neurons, activation='relu', input_shape=(X.shape[1],)),
                Dense(self.neurons // 2, activation='relu'),
                Dense(1)
            ])
            optimizer = Adam() if self.optimizer == 'adam' else SGD()
            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
            return model
    
        def fit(self, X, y):
            self.model = self.build_model()
            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs, verbose=0)
            return self
    
        def predict(self, X):
            return self.model.predict(X, verbose=0).flatten()  # Aseg√∫rate de devolver un array 1D
    
    # Crear el modelo envuelto para scikit-learn
    model = KerasRegressor()
    print("wrapper creado")
    


.. code:: ipython3

    # Definir el diccionario de hiperpar√°metros a ajustar
    param_grid = {
        'optimizer': ['adam', 'sgd'],
        'neurons': [32, 64],
        'batch_size': [16],
        'epochs': [10]
    }
    


.. code:: ipython3

    # Configurar la validaci√≥n cruzada para series temporales
    cv = TimeSeriesSplit(n_splits=3)
    
    # Configurar GridSearchCV
    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='neg_mean_squared_error', verbose=0)
    
    # Ajustar el modelo
    grid_result = grid.fit(X, y)
    
    # Mostrar los mejores par√°metros y el mejor puntaje
    print(f"Mejores par√°metros: {grid_result.best_params_}")
    print(f"Mejor puntaje (MSE): {-grid_result.best_score_}")
